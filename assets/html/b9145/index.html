<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>B9145: Topics in Trustworthy AI</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">B9145</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="syllabus.pdf">Syllabus</a></div>
<div class="menu-item"><a href="lectures.html">Lectures</a></div>
<div class="menu-item"><a href="project.html">Project</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>B9145: Topics in Trustworthy AI</h1>
<div id="subtitle"><a href="http://hsnamkoong.github.io/">Hongseok Namkoong</a>, Columbia University, Spring 2025</div>
</div>
<h2>Description</h2>
<p>Pre-trained AI systems have achieved remarkable capabilities in understanding
videos, text, and code, demonstrating reasoning abilities that match or
surpass human experts. While these omnipresent systems present unprecedented
societal opportunities, significant challenges remain before they can
meaningfully transform real-world decision-making problems.</p>
<p>A fundamental challenge is that AI systems inevitably encounter inputs unseen
during training, as they must operate continuously while processing diverse
real-world data including customer feedback and user interactions. Although
scaling datasets has improved capabilities, it has not solved this core
challenge. Modern AI systems, despite training on datasets orders of magnitude
larger than human experience, still struggle with tail inputs â€“ they
hallucinate, cannot quantify uncertainty, and perform poorly on
underrepresented groups.</p>
<p>The ability to handle tail inputs is a longstanding open problem in AI, with
limited fundamental progress over past decades. As we exhaust easily available
data sources, it is becoming clear that we must rethink the standard machine
learning paradigm. Our lack of understanding of failure modes highlights the
need for both more reliable models and rigorous safety evaluation methods.</p>
<p>This course surveys emerging topics in trustworthy machine learning, spanning
data collection, pre-training, finetuning, and inference-time methods. Most
topics discussed are active research areas, with reading materials drawn from
recent literature (to be posted on the website). The goal is to foster
discussion on new research questions, encompassing theoretical and
methodological developments, modeling considerations, novel applications, and
practical challenges.</p>
<h2>Course outline</h2>
<p>The course will comprise of pedagogical lectures and seminar-style guided
discussions. We will begin by overviewing recent advances in AI</p>
<ul>
<li><p>Architectures, optimization algorithms, and datasets</p>
</li>
<li><p>Pre-training on web-scale data</p>
</li>
<li><p>Finetuning on downstream tasks, including supervised and RL-based methods</p>
</li>
<li><p>Inference-time search methods</p>
</li>
</ul>
<p>Then, we will cover the recent set of works on improving reliability in
machine learning. Since trustworthiness is a loosely defined term with many
connotations, we will explore various aspects of this concept, alongside a
discussion of future directions. The following is a selection of topics that
will be covered in the course.</p>
<ul>
<li><p>Data-centric view of AI systems</p>
</li>
<li><p>Distribution shift</p>
</li>
<li><p>Uncertainty quantification</p>
</li>
<li><p>Adaptive data collection (active exploration)</p>
</li>
<li><p>Adversarial attacks</p>
</li>
<li><p>Fairness, equity, and data provenance</p>
</li>
<li><p>Causal learning</p>
</li>
</ul>
<h2>Lectures  </h2>
<p>Thursdays, 9am&ndash;12:15pm, Kravis 430</p>
<h2>Course staff</h2>
<p>Hongseok Namkoong (Instructor)</p>
<ul>
<li><p>Email: namkoong@gsb.columbia.edu</p>
</li>
<li><p>Office hours by appointment</p>
</li>
</ul>
<p>Daksh Mittal (TA)</p>
<ul>
<li><p>Email: DMittal27@gsb.columbia.edu </p>
</li>
</ul>
<h2>Prerequisites</h2>
<p>There are no formal prerequisites, but the class will be fast-paced and
will assume a strong background in machine learning, statistics, and
optimization. This is a class intended for PhD students conducting research in
related fields. Although some materials are of applied interest, this course
has significant theoretical content that require mathematical maturity. The
ability to read, write, and think rigorously is essential to understanding the
material.</p>
<h2>Grading  </h2>
<p>Final project (70%), class presentation (30%)</p>
<h2>Previous course offering</h2>
<p>This is a research topics class that gets significantly updated with new materials every time it is offered.
The course was last offered in <a href="./2020/index.html">Fall, 2020</a> and <a href="./2023/index.html">Spring, 2023</a>.</p>
<div id="footer">
<div id="footer-text">
Page generated 2025-01-20 18:01:19 EST, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
(<a href="index.jemdoc">source</a>)
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
