@string{tai={Trustworthy AI}}
@string{rc={Robust Causality}}
@string{aid={AI-driven Decisions}}

@string{acl = {Proceedings of the Annual Meeting of the Association for Computational Linguistics}}
@string{acm = {Association for Computing Machinery}}

@string{aistat07 = {Processing of 11th International Conference on
Artificial Intelligence and Statistics}}
@string{aistat10 = {Proceedings of the 13th International Conference
on Artificial Intelligence and Statistics}}
@string{aistat11 = {Proceedings of the 14th International Conference
on Artificial Intelligence and Statistics}}
@string{aistat12 = {Proceedings of the 15th International Conference
on Artificial Intelligence and Statistics}}
@string{aistat13 = {Proceedings of the 16th International Conference
on Artificial Intelligence and Statistics}}
@string{aistat14 = {Proceedings of the 17th International Conference
on Artificial Intelligence and Statistics}}
@string{aistat15 = {Proceedings of the 18th International Conference
on Artificial Intelligence and Statistics}}
@string{aistat16 = {Proceedings of the 19th International Conference
on Artificial Intelligence and Statistics}}
@string{aistat17 = {Proceedings of the 20th International Conference
on Artificial Intelligence and Statistics}}
@string{aistat18 = {Proceedings of the 21st International Conference
on Artificial Intelligence and Statistics}}
@string{aistat19 = {Proceedings of the 22nd International Conference
on Artificial Intelligence and Statistics}}
@string{aistat20 = {Proceedings of the 23rd International Conference
on Artificial Intelligence and Statistics}}
@string{aistat21 = {Proceedings of the 24 International Conference
on Artificial Intelligence and Statistics}}
@string{aistat23 = {Proceedings of the 26 International Conference
on Artificial Intelligence and Statistics}}

@string{colt88 = {Proceedings of the 1988 Workshop on Computational
 		  Learning Theory}}
@string{colt89 = {Proceedings of the Second Annual Workshop on Computational
		  Learning Theory}}
@string{colt90 = {Proceedings of the Third Annual Workshop on Computational
		  Learning Theory}}
@string{colt91 = {Proceedings of the Fourth Annual Workshop on
		  Computational Learning Theory}}
@string{colt92 = {Proceedings of the Fifth Annual ACM Workshop on
		  Computational Learning Theory}}
@string{colt93 = {Proceedings of the Sixth Annual ACM Conference on
		  Computational Learning Theory}}
@string{colt94 = {Proceedings of the Seventh Annual ACM Conference on
		  Computational Learning Theory}}
@string{colt95 = {Proceedings of the Eighth Annual Conference on
		  Computational Learning Theory}}
@string{colt96 = {Proceedings of the Ninth Annual Conference on
		  Computational Learning Theory}}
@string{colt97 = {Proceedings of the Tenth Annual Conference on
		  Computational Learning Theory}}
@string{colt98 = {Proceedings of the Eleventh Annual Conference on
		  Computational Learning Theory}}
@string{colt99 = {Proceedings of the Twelfth Annual Conference on
		  Computational Learning Theory}}
@string{colt00 = {Proceedings of the Thirteenth Annual Conference on
		  Computational Learning Theory}}
@string{colt01 = {Proceedings of the Fourteenth Annual Conference on
		  Computational Learning Theory}}
@string{colt02 = {Proceedings of the Fifteenth Annual Conference on
		  Computational Learning Theory}}
@string{colt03 = {Proceedings of the Sixteenth Annual Conference on
		  Computational Learning Theory}}
@string{colt04 = {Proceedings of the Seventeenth Annual Conference on
		  Computational Learning Theory}}
@string{colt05 = {Proceedings of the Eighteenth Annual Conference on
		  Computational Learning Theory}}
@string{colt06 = {Proceedings of the Nineteenth Annual Conference on
		  Computational Learning Theory}}
@string{colt07 = {Proceedings of the Twentieth Annual Conference on
		  Computational Learning Theory}}
@string{colt08 = {Proceedings of the Twenty First Annual Conference on
		  Computational Learning Theory}}
@string{colt09 = {Proceedings of the Twenty Second Annual Conference on
		  Computational Learning Theory}}
@string{colt10 = {Proceedings of the Twenty Third Annual Conference on
		  Computational Learning Theory}}
@string{colt11 = {Proceedings of the Twenty Fourth Annual Conference on
		  Computational Learning Theory}}
@string{colt12 = {Proceedings of the Twenty Fifth Annual Conference on
		  Computational Learning Theory}}
@string{colt13 = {Proceedings of the Twenty Sixth Annual Conference on
		  Computational Learning Theory}}
@string{colt14 = {Proceedings of the Twenty Seventh Annual Conference on
		  Computational Learning Theory}}
@string{colt15 = {Proceedings of the Twenty Eighth Annual Conference on
		  Computational Learning Theory}}
@string{colt16 = {Proceedings of the Twenty Ninth Annual Conference on
		  Computational Learning Theory}}
@string{colt17 = {Proceedings of the Thirtieth Annual Conference on
		  Computational Learning Theory}}
@string{colt18 = {Proceedings of the Thirty First Annual Conference on
		  Computational Learning Theory}}
@string{colt19 = {Proceedings of the Thirty Second Annual Conference on
		  Computational Learning Theory}}
@string{colt20 = {Proceedings of the Thirty Third Annual Conference on
		  Computational Learning Theory}}


@string{ecp = {Electronic Communications in Probability}}
@string{emnlp = {Proceedings of Empirical Methods for Natural Language
                 Processing}}

@string{eurocolt93 = {Computational Learning Theory: EuroCOLT '93}}
@string{eurocolt95 = {Computational Learning Theory: Second European
			Conference, EuroCOLT~'95}}
@string{eurocolt99 = {Computational Learning Theory: Fourth European
			Conference, EuroCOLT~'99}}

@string{esann99 = {Proceedings of the Seventh European Symposium on
                   Artificial Neural Networks}}

@string{focs78 = {19th Annual Symposium on Foundations of Computer Science}}
@string{focs79 = {20th Annual Symposium on Foundations of Computer Science}}
@string{focs80 = {21st Annual Symposium on Foundations of Computer Science}}
@string{focs81 = {22nd Annual Symposium on Foundations of Computer Science}}
@string{focs82 = {23rd Annual Symposium on Foundations of Computer Science}}
@string{focs83 = {24th Annual Symposium on Foundations of Computer Science}}
@string{focs84 = {25th Annual Symposium on Foundations of Computer Science}}
@string{focs85 = {26th Annual Symposium on Foundations of Computer Science}}
@string{focs86 = {27th Annual Symposium on Foundations of Computer Science}}
@string{focs87 = {28th Annual Symposium on Foundations of Computer Science}}
@string{focs88 = {29th Annual Symposium on Foundations of Computer Science}}
@string{focs89 = {30th Annual Symposium on Foundations of Computer Science}}
@string{focs90 = {31st Annual Symposium on Foundations of Computer Science}}
@string{focs91 = {32nd Annual Symposium on Foundations of Computer Science}}
@string{focs92 = {33rd Annual Symposium on Foundations of Computer Science}}
@string{focs93 = {34th Annual Symposium on Foundations of Computer Science}}
@string{focs94 = {35th Annual Symposium on Foundations of Computer Science}}
@string{focs95 = {36th Annual Symposium on Foundations of Computer Science}}
@string{focs96 = {37th Annual Symposium on Foundations of Computer Science}}
@string{focs97 = {38th Annual Symposium on Foundations of Computer Science}}
@string{focs98 = {39th Annual Symposium on Foundations of Computer Science}}
@string{focs99 = {40th Annual Symposium on Foundations of Computer Science}}
@string{focs00 = {41st Annual Symposium on Foundations of Computer Science}}
@string{focs01 = {42nd Annual Symposium on Foundations of Computer Science}}
@string{focs02 = {43rd Annual Symposium on Foundations of Computer Science}}
@string{focs03 = {44th Annual Symposium on Foundations of Computer Science}}
@string{focs04 = {45th Annual Symposium on Foundations of Computer Science}}
@string{focs05 = {46th Annual Symposium on Foundations of Computer Science}}
@string{focs06 = {47th Annual Symposium on Foundations of Computer Science}}
@string{focs07 = {48th Annual Symposium on Foundations of Computer Science}}
@string{focs08 = {49th Annual Symposium on Foundations of Computer Science}}
@string{focs09 = {50th Annual Symposium on Foundations of Computer Science}}
@string{focs10 = {51st Annual Symposium on Foundations of Computer Science}}
@string{focs11 = {52nd Annual Symposium on Foundations of Computer Science}}
@string{focs12 = {53rd Annual Symposium on Foundations of Computer Science}}
@string{focs13 = {54th Annual Symposium on Foundations of Computer Science}}
@string{focs14 = {55th Annual Symposium on Foundations of Computer Science}}
@string{focs15 = {56th Annual Symposium on Foundations of Computer Science}}
@string{focs16 = {57th Annual Symposium on Foundations of Computer Science}}

@string{recomb19 = {23rd Annual International Conference on Research
  in Computational Molecular Biology (RECOMB)}}
  
@string{soda90 = {Proceedings of the First Annual ACM-SIAM Symposium on
		  Discrete Algorithms (SODA)}}
@string{soda91 = {Proceedings of the Second Annual ACM-SIAM Symposium on
		  Discrete Algorithms (SODA)}}
@string{soda07 = {Proceedings of the Eighteenth ACM-SIAM Symposium on
		  Discrete Algorithms (SODA)}}
@string{soda10 = {Proceedings of the Twenty-First ACM-SIAM Symposium on
		  Discrete Algorithms (SODA)}}
@string{soda13 = {Proceedings of the Twenty-Fourth ACM-SIAM Symposium on
		  Discrete Algorithms (SODA)}}
@string{soda14 = {Proceedings of the Twenty-Fifth ACM-SIAM Symposium on
		  Discrete Algorithms (SODA)}}
@string{soda18 = {Proceedings of the Twenty-Ninth ACM-SIAM 
Symposium on Discrete Algorithms (SODA)}}
@string{soda19 = {Proceedings of the Thirtieth ACM-SIAM Symposium on
		  Discrete Algorithms (SODA)}}

@string{stoc79 = {Proceedings of the Eleventh Annual ACM Symposium on
		  Theory of Computing}}
@string{stoc80 = {Proceedings of the Twelfth Annual ACM Symposium on
		  Theory of Computing}}
@string{stoc81 = {Proceedings of the Thirteenth Annual ACM Symposium on
		  Theory of Computing}}
@string{stoc82 = {Proceedings of the Fourteenth Annual ACM Symposium on
		  Theory of Computing}}
@string{stoc83 = {Proceedings of the Fifteenth Annual ACM Symposium on
		  Theory of Computing}}
@string{stoc84 = {Proceedings of the Sixteenth Annual ACM Symposium on
		  Theory of Computing}}
@string{stoc85 = {Proceedings of the Seventeenth Annual ACM Symposium on
		  Theory of Computing}}
@string{stoc86 = {Proceedings of the Eighteenth Annual ACM Symposium on
		  Theory of Computing}}
@string{stoc87 = {Proceedings of the Nineteenth Annual ACM Symposium on
		  Theory of Computing}}
@string{stoc88 = {Proceedings of the Twentieth Annual ACM Symposium on
		  Theory of Computing}}
@string{stoc89 = {Proceedings of the Twenty First Annual ACM Symposium on
		  Theory of Computing}}
@string{stoc90 = {Proceedings of the Twenty Second Annual ACM Symposium on
		  Theory of Computing}}
@string{stoc91 = {Proceedings of the Twenty Third Annual ACM Symposium on
		  Theory of Computing}}
@string{stoc92 = {Proceedings of the Twenty-Fourth Annual ACM
		  Symposium on the Theory of Computing}}
@string{stoc93 = {Proceedings of the Twenty-Fifth Annual ACM
		  Symposium on the Theory of Computing}}
@string{stoc94 = {Proceedings of the Twenty-Sixth Annual ACM
		  Symposium on the Theory of Computing}}
@string{stoc95 = {Proceedings of the Twenty-Seventh Annual ACM
		  Symposium on the Theory of Computing}}
@string{stoc96 = {Proceedings of the Twenty-Eighth Annual ACM
		  Symposium on the Theory of Computing}}
@string{stoc97 = {Proceedings of the Twenty-Ninth Annual ACM
		  Symposium on the Theory of Computing}}
@string{stoc98 = {Proceedings of the Thirtieth Annual ACM
		  Symposium on the Theory of Computing}}
@string{stoc99 = {Proceedings of the Thirty-First Annual ACM
	  	Symposium on the Theory of Computing}}
@string{stoc02 = {Proceedings of the Thirty-Fourth Annual ACM
		  Symposium on the Theory of Computing}}
@string{stoc07 = {Proceedings of the Thirty-Ninth Annual ACM
		  Symposium on the Theory of Computing}}
@string{stoc08 = {Proceedings of the Fortieth Annual ACM
		  Symposium on the Theory of Computing}}
@string{stoc09 = {Proceedings of the Forty-First Annual ACM
		  Symposium on the Theory of Computing}}
@string{stoc10 = {Proceedings of the Forty-Second Annual ACM
		  Symposium on the Theory of Computing}}
@string{stoc11 = {Proceedings of the Forty-Third Annual ACM
		  Symposium on the Theory of Computing}}
@string{stoc12 = {Proceedings of the Forty-Fourth Annual ACM
		  Symposium on the Theory of Computing}}
@string{stoc13 = {Proceedings of the Forty-Fifth Annual ACM
		  Symposium on the Theory of Computing}}
@string{stoc14 = {Proceedings of the Forty-Sixth Annual ACM
		  Symposium on the Theory of Computing}}
@string{stoc15 = {Proceedings of the Forty-Seventh Annual ACM
		  Symposium on the Theory of Computing}}
@string{stoc16 = {Proceedings of the Forty-Eighth Annual ACM
		  Symposium on the Theory of Computing}}
@string{stoc17 = {Proceedings of the Forty-Ninth Annual ACM
		  Symposium on the Theory of Computing}}
@string{stoc18 = {Proceedings of the Fiftieth Annual ACM
		  Symposium on the Theory of Computing}}
@string{stoc19 = {Proceedings of the Fifty-first Annual ACM
		  Symposium on the Theory of Computing}}

@string{pods02 = {Proceedings of the Twenty-First ACM SIGMOD-SIGACT-SIGART
  Symposium on Principles of Database Systems}}
@string{pods10 = {Proceedings of the Twenty-Ningth ACM SIGMOD-SIGACT-SIGART
  Symposium on Principles of Database Systems}}

@string{ml94 =	{Machine Learning: Proceedings of the Eleventh
		 International Conference}}
@string{ml95 =	{Proceedings of the Twelfth International Conference
		 on Machine Learning}}
@string{ml96=	{Machine Learning: Proceedings of the Thirteenth
		 International Conference}}
@string{ml97=	{Machine Learning: Proceedings of the Fourteenth
		 International Conference}}
@string{ml98=	{Machine Learning: Proceedings of the Fifteenth
		 International Conference}}
@string{ml99=	{Machine Learning: Proceedings of the Sixteenth
		 International Conference}}
@string{ml00=	{Machine Learning: Proceedings of the Seventeenth
		 International Conference}}
@string{ml02=	{Machine Learning: Proceedings of the Nineteenth
		 International Conference}}

@string{ecml94=	{Machine Learning: ECML-94}}
@string{ecml=	{European Conference on Machine Learning}}

@string{itcs2017 = {Proceedings of the Eighth Innovations in Theoretical Computer Science (ITCS)}}
@string{itcs2019 = {Proceedings of the Tenth Innovations in Theoretical Computer Science (ITCS)}}


@string{aaai97= {Proceedings of the Fourteenth National Conference on
		  Artificial Intelligence}}
@string{aaai98= {Proceedings of the Fifteenth National Conference on
		  Artificial Intelligence}}
@string{aaai99= {Proceedings of the Sixteenth National Conference on
		  Artificial Intelligence}}
@string{aaai15= {Proceedings of the Thirty-Second National Conference on
		  Artificial Intelligence}}
@string{aaai16= {Proceedings of the Thirty-Third AAAI Conference on
		  Artificial Intelligence}}
@string{aaai17= {Proceedings of the Thirty-Fourth AAAI Conference on
		  Artificial Intelligence}}
@string{aaai18= {Proceedings of the Thirty-Fifth AAAI Conference on
		  Artificial Intelligence}}
@string{aaai19= {Proceedings of the Thirty-Sixth AAAI Conference on
		  Artificial Intelligence}}
@string{aaai20= {Proceedings of the Thirty-Seventh AAAI Conference on
		  Artificial Intelligence}}
@string{aaai21= {Proceedings of the Thirty-Eighth AAAI Conference on
		  Artificial Intelligence}}

@string{anprob=	{Annals of Probability}}
@string{anappprob={Annals of Applied Probability}}
@string{aoap = {Annals of Applied Probability}}
@string{annstat={The Annals of Statistics}}
@string{aoms = {Annals of Mathematical Statistics}}
@string{aos = {Annals of Statistics}}
@string{aoas = {Annals of Applied Statistics}}
@string{aop = {Annals of Probability}}
@string{bams = {Bulletin of the American Mathematical Society}}
@string{cacm =	{Communications of the ACM}}
@string{eurosam79={Symbolic and Algebraic Computation}}
@string{ejs={Electronic Journal of Statistics}}
@string{icalp92 = {Automata, Languages and Programming: 19th
		   International Colloquium}}
@string{ijcai85 = {Proceedings of the 9th International Joint
		   Conference on Artificial Intelligence}}
@string{ijcai03 = {Proceedings of the 18th International Joint
                   Conference on Artificial Intelligence}}
@string{infcomp={Information and Computation}}
@string{infctrl={Information and Control}}
@string{ieeeit=	{IEEE Transactions on Information Theory}}
@string{ieeepami= {IEEE Transactions on Pattern Analysis and Machine Intelligence}}
@string{ieeenn=	{IEEE Transactions on Neural Networks}}
@string{ieeesp = {IEEE Transactions on Signal Processing}}
@string{ieeetac= {IEEE Transactions on Automatic Control}}
@string{jacm =	{Journal of the Association for Computing Machinery}}
@string{jair =	{Journal of Artificial Intelligence Research}}
@string{jams =	{Journal of the American Mathematical Society}}
@string{jasa =	{Journal of the American Statistical Association}}
@string{jcss = 	{Journal of Computer and System Sciences}}
@string{jmlr =	{Journal of Machine Learning Research}}
@string{jrss = {Journal of the Royal Statistical Society}}
@string{jrssb = {Journal of the Royal Statistical Society, Series B}}
@string{mathor = {Mathematics of Operations Research}}
@string{mathprog = {Mathematical Programming}}
@string{mathprogc = {Mathematical Programming Computation}}
@string{mathproga = {Mathematical Programming, Series A}}
@string{mathprogb = {Mathematical Programming, Series B}}
@string{mathprog = {Mathematical Programming}}
@string{mit =	{Massachusetts Institute of Technology}}
@string{mitlcs=	{MIT Laboratory for Computer Science}}
@string{ml =	{Machine Learning}}
@string{ms = {Management Science}}
@string{or = {Operations Research}}
@string{pnas =  {Proceedings of the National Academy of Sciences}}
@string{ptrf = {Probability Theory and Related Fields}}
@string{sicomp ={SIAM Journal on Computing}}
@string{statsci = {Statistical Science}}
@string{stochsys ={Stochastic Systems}}
@string{symcomp={Journal of Symbolic Computation}}
@string{tams = {Transactions of the American Mathematical Society}}
@string{tcs =	{Theoretical Computer Science}}
@string{ucsccrl={University of California Santa Cruz,
		 Computer Research Laboratory}}

@string{kdd10 = {Proceedings of the 16th ACM SIGKDD Conference on Knowledge
   Discovery and Data Mining (KDD)}},
@string{kdd11 = {Proceedings of the 17th ACM SIGKDD Conference on Knowledge
   Discovery and Data Mining (KDD)}},
@string{kdd21 = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge
   Discovery and Data Mining (KDD)}},
@string{kdd23 = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge
   Discovery and Data Mining (KDD)}},
 
%% NOTE: NeurIPS has an off-by-one error in its numbering. The
%% Proceedings of the Thirty-Second Conference on Neural Information
%% Processing Systems is the *book* Advances in Neural Information
%% Processing Systems 31.
@string{nips1=	{Advances in Neural Information Processing Systems 1}}
@string{nips5=	{Advances in Neural Information Processing Systems 5}}
@string{nips7=	{Advances in Neural Information Processing Systems 7}}
@string{nips8=	{Advances in Neural Information Processing Systems 8}}
@string{nips10=	{Advances in Neural Information Processing Systems 10}}
@string{nips11=	{Advances in Neural Information Processing Systems 11}}
@string{nips12=	{Advances in Neural Information Processing Systems 12}}
@string{nips13=	{Advances in Neural Information Processing Systems 13}}
@string{nips14=	{Advances in Neural Information Processing Systems 14}}
@string{nips15=	{Advances in Neural Information Processing Systems 15}}
@string{nips16=	{Advances in Neural Information Processing Systems 16}}
@string{nips17=	{Advances in Neural Information Processing Systems 17}}
@string{nips18=	{Advances in Neural Information Processing Systems 18}}
@string{nips19= {Advances in Neural Information Processing Systems 19}}
@string{nips20= {Advances in Neural Information Processing Systems 20}}
@string{nips21= {Advances in Neural Information Processing Systems 21}} % 2007
@string{nips22= {Advances in Neural Information Processing Systems 22}} % 2008
@string{nips23= {Advances in Neural Information Processing Systems 23}} % 2009
@string{nips2010= {Advances in Neural Information Processing Systems 23}}
@string{nips2011= {Advances in Neural Information Processing Systems 24}}
@string{nips2012= {Advances in Neural Information Processing Systems 25}} % 2012
@string{nips2013= {Advances in Neural Information Processing Systems 26}} % 2013
@string{nips2014= {Advances in Neural Information Processing Systems 27}} % 2014
@string{nips2015= {Advances in Neural Information Processing Systems 28}} % 2015
@string{nips2016= {Advances in Neural Information Processing Systems 29}} % 2016
@string{nips2017= {Advances in Neural Information Processing Systems 30}} % 2017
@string{nips2018= {Advances in Neural Information Processing Systems 31}} % 2018
@string{nips2019= {Advances in Neural Information Processing Systems 32}} % 2019
@string{nips2020= {Advances in Neural Information Processing Systems 33}} % 2020
@string{nips2021= {Advances in Neural Information Processing Systems 34}} % 2021
@string{nips2022= {Advances in Neural Information Processing Systems 35}} % 2022
@string{nips2023= {Advances in Neural Information Processing Systems 36}} % 2023
@string{nips2024= {Advances in Neural Information Processing Systems 37}} % 2024
@string{nips2024db= {Advances in Neural Information Processing Systems 37, Datasets and Benchmark Track}} % 2024
@string{nips2025= {Advances in Neural Information Processing Systems 38}} % 2025

@string{nips5eds={Stephen Jos\'e Hanson and Jack D. Cowan and C. Lee Giles}}

@string{sigir88={Proceedings of the 11th Annual International
		  ACM SIGIR Conference on Research and Development in
		  Information Retrieval}}
@string{sigir94={Proceedings of the 17th Annual International
		  ACM SIGIR Conference on Research and Development in
		  Information Retrieval}}
@string{sigir95={Proceedings of the 18th Annual International
		  ACM SIGIR Conference on Research and Development in
		  Information Retrieval}}
@string{sigir96={Proceedings of the 19th Annual International
		  ACM SIGIR Conference on Research and Development in
		  Information Retrieval}}
@string{sigir97={Proceedings of the 20th Annual International
		  ACM SIGIR Conference on Research and Development in
		  Information Retrieval}}
@string{sigir02={Proceedings of the 25th Annual International
		  ACM SIGIR Conference on Research and Development in
		  Information Retrieval}}

@string{simat={SIAM Journal on Matrix Analysis and Applications}}
@string{siopt={SIAM Journal on Optimization}}
@string{sicon={SIAM Journal on Control and Optimization}}

@string{www09 ={Proceedings of the 18th International Conference on World
               Wide Web, WWW 2009, Madrid, Spain, April 20-24, 2009}}

@string{icml96 = {Proceedings of the Thirteenth International Conference on Machine Learning}}
@string{icml97 = {Proceedings of the Fourteenth International Conference on Machine Learning}}
@string{icml98 = {Proceedings of the Fifteenth International Conference on Machine Learning}}
@string{icml99 = {Proceedings of the Sixteenth International Conference on Machine Learning}}
@string{icml00 = {Proceedings of the Seventeenth International Conference on Machine Learning}}
@string{icml01 = {Proceedings of the Eighteenth International Conference on Machine Learning}}
@string{icml02 = {Proceedings of the Nineteenth International Conference on Machine Learning}}
@string{icml03 = {Proceedings of the Twentieth International Conference on Machine Learning}}
@string{icml04 = {Proceedings of the Twenty-First International Conference on Machine Learning}}
@string{icml05 = {Proceedings of the 22nd International Conference on Machine Learning}}
@string{icml06 = {Proceedings of the 23rd International Conference on Machine Learning}}
@string{icml07 = {Proceedings of the 24th International Conference on Machine Learning}}
@string{icml08 = {Proceedings of the 25th International Conference on Machine Learning}}
@string{icml09 = {Proceedings of the 26th International Conference on Machine Learning}}
@string{icml10 = {Proceedings of the 27th International Conference on Machine Learning}}
@string{icml11 = {Proceedings of the 28th International Conference on Machine Learning}}
@string{icml12 = {Proceedings of the 29th International Conference on Machine Learning}}
@string{icml13 = {Proceedings of the 30th International Conference on Machine Learning}}
@string{icml14 = {Proceedings of the 31st International Conference on Machine Learning}}
@string{icml15 = {Proceedings of the 32nd International Conference on Machine Learning}}
@string{icml16 = {Proceedings of the 33rd International Conference on Machine Learning}}
@string{icml17 = {Proceedings of the 34th International Conference on Machine Learning}}
@string{icml18 = {Proceedings of the 35th International Conference on Machine Learning}}
@string{icml19 = {Proceedings of the 36th International Conference on Machine Learning}}
@string{icml20 = {Proceedings of the 37th International Conference on Machine Learning}}
@string{icml21 = {Proceedings of the 38th International Conference on Machine Learning}}
@string{icml22 = {Proceedings of the 39th International Conference on Machine Learning}}
@string{icml23 = {Proceedings of the 40th International Conference on Machine Learning}}
@string{icml24 = {Proceedings of the 41st International Conference on Machine Learning}}
@string{icml25 = {Proceedings of the 42nd International Conference on Machine Learning}}


@string{iclr14 = {Proceedings of the Second International Conference on 
Learning Representations}}
@string{iclr15 = {Proceedings of the Third International Conference on Learning Representations}}
@string{iclr16 = {Proceedings of the Fourth International Conference on 
Learning Representations}}
@string{iclr17 = {Proceedings of the Fifth International Conference on Learning Representations}}
@string{iclr18 = {Proceedings of the Sixth International Conference on 
Learning Representations}}
@string{iclr19 = {Proceedings of the Seventh International Conference on 
Learning Representations}}
@string{iclr20 = {Proceedings of the Eighth International Conference on 
Learning Representations}}
@string{iclr21 = {Proceedings of the Ninth International Conference on 
Learning Representations}}
@string{iclr22 = {Proceedings of the Tenth International Conference on 
Learning Representations}}
@string{iclr23 = {Proceedings of the Eleventh International Conference on 
Learning Representations}}
@string{iclr24 = {Proceedings of the Twelveth International Conference on 
Learning Representations}}


@string{uai98 = {Proceedings of the Fourteenth Conference on Uncertainty
  in Artificial Intelligence}}
@string{uai99 = {Proceedings of the Fifteenth Conference on Uncertainty
  in Artificial Intelligence}}
@string{uai00 = {Proceedings of the Sixteenth Conference on Uncertainty
  in Artificial Intelligence}}
@string{uai01 = {Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence}}
@string{uai02 = {Proceedings of the Eighteenth Conference on Uncertainty
  in Artificial Intelligence}}
@string{uai03 = {Proceedings of the Nineteenth Conference on Uncertainty
  in Artificial Intelligence}}
@string{uai04 = {Proceedings of the 20th Conference on Uncertainty
  in Artificial Intelligence}}
@string{uai05 = {Proceedings of the 21st Conference on Uncertainty
  in Artificial Intelligence}}
@string{uai06 = {Proceedings of the 22nd Conference on Uncertainty
  in Artificial Intelligence}}
@string{uai07 = {Proceedings of the 23rd Conference on Uncertainty
  in Artificial Intelligence}}
@string{uai08 = {Proceedings of the 24th Conference on Uncertainty
  in Artificial Intelligence}}
@string{uai09 = {Proceedings of the 25th Conference on Uncertainty
  in Artificial Intelligence}}
@string{uai10 = {Proceedings of the 26th Conference on Uncertainty
  in Artificial Intelligence}}
@string{uai11 = {Proceedings of the 27th Conference on Uncertainty
  in Artificial Intelligence}}



@string{allerton10 = {The 48th Allerton Conference on Communication, Control,
 and Computing}}
@string{allerton11 = {The 49th Allerton Conference on Communication, Control,
 and Computing}}
@string{allerton12 = {The 50th Allerton Conference on Communication, Control,
 and Computing}}
@string{allerton15 = {The 53rd Allerton Conference on Communication, Control,
 and Computing}}
@string{allerton16 = {The 54th Allerton Conference on Communication, Control,
 and Computing}}
@string{allerton17 = {The 55th Allerton Conference on Communication, Control,
 and Computing}}

@string{cdc1974 = {13th IEEE Conference on Decisions and Control}}
@string{cdc2011 = {50th IEEE Conference on Decisions and Control}}
@string{cdc2012 = {51st IEEE Conference on Decisions and Control}}
@string{cdc2013 = {52nd IEEE Conference on Decisions and Control}}
@string{cdc2014 = {53rd IEEE Conference on Decisions and Control}}
@string{cdc2015 = {54th IEEE Conference on Decisions and Control}}
@string{cdc2016 = {55th IEEE Conference on Decisions and Control}}
@string{cdc2017 = {56th IEEE Conference on Decisions and Control}}
@string{cdc2018 = {57th IEEE Conference on Decisions and Control}}

@string{cvpr16 = "Proceedings of the 26th IEEE Conference on Computer Vision and Pattern Recognition"}
@string{cvpr17 = "Proceedings of the 27th IEEE Conference on Computer Vision and Pattern Recognition"}
@string{cvpr18 = "Proceedings of the 28th IEEE Conference on Computer Vision and Pattern Recognition"}
@string{cvpr19 = "Proceedings of the 29th IEEE Conference on Computer Vision and Pattern Recognition"}
@string{cvpr20 = "Proceedings of the 30th IEEE Conference on Computer Vision and Pattern Recognition"}
@string{cvpr21 = "Proceedings of the 31st IEEE Conference on Computer Vision and Pattern Recognition"}
@string{cvpr22 = "Proceedings of the 32nd IEEE Conference on Computer Vision and Pattern Recognition"}


@string{winsim16 = {Proceedings of the 2016 Winter Simulation Conference}}
@string{winsim17 = {Proceedings of the 2017 Winter Simulation Conference}}

@string{cpam = {Communications on Pure and Applied Mathematics}}
@string{focm = {Foundations of Computational Mathematics}}
@string{wiley=	{John Wiley \& Sons}}
@string{cvpr = "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"}
@string{iccv = "Proceedings of the International Conference on Computer Vision"}
@string{eccv = "Proceedings of the European Conference on Computer Vision"}
@string{icip = "Proceedings of the IEEE International Conference on Image Processing"}
@string{siopt = {SIAM Journal on Optimization}}
@string{jota = {Journal of Optimization Theory and Applications}}

@string{or = {Operations Research}}
@string{ms = {Management Science}}
@string{mathor = {Mathematics of Operations Research}}
@string{stochsys = {Stochastic Systems}}
@string{poms = {Production and Operations Management}}
@string{msom = {Manufacturing and Service Operations Management}}
@string{qs = {Queueing Systems}}

@article{YangChYeNa25,
      title={Benchmarking In-context Experiential Learning Through Repeated Product Recommendations}, 
      author={Gilbert Yang and Yaqin Chen and Thomson Yen and Hongseok Namkoong},
      journal={arXiv:2511.22130 [cs.LG]},
      abbr=aid,
      bibtex_show={true},
      arxiv={2511.22130},
      selected={false},
      url={https://arxiv.org/abs/2511.22130}, 
      code={https://github.com/namkoong-lab/interactive-benchmark},
      website={https://www.experiential-learning-benchmark.com/}
}


@article{CastellaniYeMiYeNa25,
      title={SynthTools: A Framework for Scaling Synthetic Tools for Agent Development}, 
      author={Castellani, Tommaso and Ye, Naimeng and Mittal, Daksh and Yen, Thomson and Namkoong, Hongseok},
      journal={arXiv:2511.09572 [cs.AI]},
      year={2025},
      abbr=aid,
      bibtex_show={true},
      arxiv={2511.09572},
      selected={false},
      url={https://arxiv.org/abs/2511.09572}, 
}

@article{MaNa25,
      title={A Sensitivity Approach to Causal Inference Under Limited Overlap}, 
      author={Yuanzhe Ma and Hongseok Namkoong},
      journal={arXiv:2511.22003 [stat.ML]},
      year={2025},
      abbr=tai,
      bibtex_show={true},
      arxiv={2511.22003},
      selected={false},
      url={https://arxiv.org/abs/2511.22003}, 
}


@article{QuNaZe25,
      title={A Broader View of Thompson Sampling}, 
      author={Qu, Yanlin and Namkoong, Hongseok and Zeevi, Assaf},
      journal={arxiv:2510.07208 [cs.LG]},
      year={2025},
      abbr=aid,
      bibtex_show={true},
      arxiv={2510.07208},
      selected={false},
      url={https://arxiv.org/abs/2510.07208}, 
}

@article{MittalZhDoNa25,
  title={Data-driven Stochastic Modeling using Autoregressive Sequence Models},
  author={Mittal, Daksh and Zheng, Shunri and Dong, Jing and Namkoong, Hongseok},
  journal={arxiv:2509.05839 [cs.LG]},
  year={2025},
  abbr=aid,
  bibtex_show={true},
  arxiv={2509.05839},
  selected={true},
  url={https://arxiv.org/abs/2509.05839}
}

@article{LiuWaLaNaBl25,
  title={{DRO}: A {P}ython Library for Distributionally Robust Optimization in Machine Learning},
  author={Liu, Jiashuo and Wang, Tianyu and Lam, Henry and Namkoong, Hongseok and Blanchet, Jose},
  journal={arXiv:2505.23565 [cs.LG]},
  year={2025},
  abbr=tai,
  bibtex_show={true},
  arxiv={arXiv:2505.23565},
  selected={false},
  url={https://arxiv.org/abs/2505.23565},
  code={https://github.com/namkoong-lab/dro}
}


@article{KobayashiJiYaNaJo25,
      title={Learning-To-Measure: In-context Active Feature Acquisition}, 
      author={Yuta Kobayashi and Zilin Jing and Jiayu Yao and Hongseok Namkoong and Shalmali Joshi},
      year={2025},
      journal={arxiv:2510.12624 [cs.LG]},
      abbr=tai,
      bibtex_show={true},
      arxiv={2510.12624},
      selected={false},
      url={https://arxiv.org/abs/2510.12624}, 
}

@article{HuEtAl,
  title={FinSearchComp: Towards a Realistic, Expert-Level Evaluation of Financial Search and Reasoning},
  author={Hu, Liang and Jiao, Jianpeng and Liu, Jiashuo and Ren, Yanle and Wen, Zhoufutu and Zhang, Kaiyuan and Zhang, Xuanliang and Gao, Xiang and He, Tianci and Hu, Fei and Liao, Yali and Wang, Zaiyuan and Yang, Chenghao and Yang, Qianyu and Yin, Mingren and Zeng, Zhiyuan and Zhang, Ge and Zhang, Xinyi and Zhao, Xiying and Zhu, Zhenwei and Namkoong, Hongseok and Huang, Wenhao and Tang, Yuwen},
  journal={arXiv:2509.13160 [cs.LG]},
  year={2025},
  abbr=aid,
  bibtex_show={true},
  arxiv={arXiv:2509.13160},
  selected={false},
  url={https://arxiv.org/abs/2509.13160},
  code={https://randomtutu.github.io/FinSearchComp/}
}


@inproceedings{WangZoZeNa25,
  title={Adaptive Elicitation of Latent Information Using Natural Language},
  author={Wang$*$, Jimmy and Zollo$*$, Thomas and Zemel, Richard and Namkoong, Hongseok},
  booktitle=icml25,
  year={2025},
  abbr=aid,
  bibtex_show={true},
  arxiv={2504.04204},
  url={https://arxiv.org/abs/2504.04204},
  code={https://github.com/namkoong-lab/adaptive-elicitation},
  note={Best Paper Award at ICLR 2025 Workshop on Quantify Uncertainty and Hallucination in Foundation Models}
}

@inproceedings{YenSiChPeGuNa25,
  title={Data Mixture Optimization: A Multi-fidelity Multi-scale Bayesian Framework},
  author={Yen$*$, Thomson and Siah$*$, Andrew Wei Tung and Chen, Haozhe and Peng, Tianyi and Guetta, Daniel and Namkoong, Hongseok},
  booktitle=nips2025,
  year={2025},
  abbr=tai,
  bibtex_show={true},
  arxiv={2503.21023},
  selected={false},
  url={https://arxiv.org/abs/2503.21023},
  abstract={A central challenge in AI is deciding how best to combine different data sources when training large models, as the performance of these models strongly depends on the composition of their training data. Historically, choosing optimal data mixtures has relied on intuition or expensive trial-and-error, limiting systematic progress even for resource-rich corporations. This research introduces a significant intellectual innovation: a novel Bayesian optimization framework based on probabilistic scaling laws that explicitly models uncertainty in how small-scale experiments inform large-scale training decisions. Enabled by substantial computational resources afforded by Empire AI, this framework adaptively selects data mixtures, model sizes, and training durations, efficiently balancing cost and information gain. By replacing brittle deterministic scaling laws with a flexible, probabilistic approach, this research enables optimal data curation through principled decision-making. Ultimately, this work sets a new scientific foundation for data mixture optimization, broadly advancing the efficiency, accessibility, and effectiveness of AI development across sectors.},
  code={https://github.com/namkoong-lab/data-recipes}
}

@inproceedings{LiChNaPe25,
  title={LLM Generated Persona is a Promise with a Catch},
  author={Li, Ang and Chen, Haozhe and Namkoong, Hongseok and Peng, Tianyi},
  booktitle=nips2025,
  year={2025},
  abbr=tai,
  bibtex_show={true},
  arxiv={2503.16527},
  url={https://arxiv.org/abs/2503.16527},
  abstract={Large language model (LLM)-generated digital twins—synthetic personas that simulate real-world human behavior—have the potential to replace costly, time-intensive surveys with scalable, dynamic simulations. These “digital twins” can provide unprecedented insights into consumer behavior, political trends, and social dynamics, enabling businesses and researchers to test ideas, optimize products, and forecast societal shifts with greater speed and precision. However, this promise comes with a fundamental challenge: current LLM-driven persona generation methods lack rigor, introducing systematic biases that can misrepresent population-level behaviors and lead to flawed conclusions. This study reveals that widely used heuristic approaches to persona creation fail to capture the complexity of real-world diversity, sometimes producing distortions significant enough to alter election forecasts and public opinion studies. Addressing these flaws requires a scientific approach to persona generation—grounded in empirical validation, methodological innovation, and interdisciplinary collaboration—to ensure these digital twins accurately reflect human populations. This work was made possible by the computational infrastructure Empire AI provides, providing an order of magnitude more computational power than previous studies, enabling large-scale analysis of biases and reliability in LLM-generated personas.},
  code={https://huggingface.co/datasets/Tianyi-Lab/Personas}
}

@inproceedings{MittalLiYeGuNa25,
  title={Architectural and Inferential Inductive Biases For Exchangeable Sequence Modeling},
  author={Mittal$*$, Daksh and Li$*$, Ang and Yen$*$, Tzu-Ching and Guetta, Daniel and Namkoong, Hongseok},
  booktitle=nips2025,
  year={2025},
  abbr=aid,
  bibtex_show={true},
  arxiv={2502.06076},
  url={https://arxiv.org/abs/2503.01215}
}

@article{MittalMaJoNa25,
  title={A Planning Framework for Adaptive Labeling},
  author={Mittal$*$, Daksh and Ma$*$, Yuanzhe and Joshi, Shalmali and Namkoong, Hongseok},
  journal={arXiv:2502.06076 [stat.ML]},
  year={2025},
  abbr=tai,
  note = {Journal version under review; Conference version appeared in NeurIPS 2024},
  bibtex_show={true},
  arxiv={2502.06076},
  url={http://arxiv.org/abs/2502.06076}
}

@inproceedings{ZhangCaNaRu25,
  title={Contextual Thompson Sampling via Generation of Missing Data},
  author={Zhang, Kelly and Cai, Tiffany and Namkoong, Hongseok and Russo, Daniel},
  booktitle=nips2025,
  year={2025},
  abbr=aid,
    bibtex_show={true},
  arxiv={2502.07064},
  url={https://arxiv.org/abs/2502.07064}
}

@article{JoshiEtAl25,
    author = {Joshi, Shalmali and Urteaga, Iñigo and van Amsterdam, Wouter A C and Hripcsak, George and Elias, Pierre and Recht, Benjamin and Elhadad, Noémie and Fackler, James and Sendak, Mark P and Wiens, Jenna and Deshpande, Kaivalya and Wald, Yoav and Fiterau, Madalina and Lipton, Zachary and Malinsky, Daniel and Nayan, Madhur and Namkoong, Hongseok and Park, Soojin and Vogt, Julia E and Ranganath, Rajesh},
    title = {{AI} as an intervention: improving clinical outcomes relies on a causal approach to {AI} development and validation},
    journal = {Journal of the American Medical Informatics Association},
    volume = {32},
    number = {3},
    pages = {589-594},
    year = {2025},
    abbr=aid,
    bibtex_show={true},
    url={https://academic.oup.com/jamia/article/32/3/589/7945189}
}


@inproceedings{HsuDiSiNa24,
      title={From Models to Systems: A Comprehensive Framework for AI System Fairness in Compositional Recommender Systems}, 
      author={Brian Hsu and Cyrus DiCiccio and Natesh Sivasubramoniapillai and Hongseok Namkoong},
      year={2024},
      booktitle = 	 {Proceedings of the Algorithmic Fairness Through the Lens of Metrics and Evaluation},
      pages = 	 {8--37},
      abbr=tai,
      bibtex_show={true},
      arxiv={2412.04655},
      url={https://proceedings.mlr.press/v279/hsu25a.html},
      volume = 	 {279},
      series = 	 {Proceedings of Machine Learning Research},
      publisher =    {PMLR},
      selected={false}
}

@article{ZengLiLaNa24,
  title={{LLM} Embeddings Improve Test-time Adaptation to Tabular {$Y|X$}-Shifts},
  author={Zeng$*$, Yibo and Liu$*$, Jiashuo and Lam, Henry and Namkoong, Hongseok},
  journal={arXiv:2410.07395 [cs.LG]},
  year={2024},
  abbr=tai,
  bibtex_show={true},
  arxiv={2410.07395},
  url={https://arxiv.org/abs/2410.07395},
  code={https://github.com/namkoong-lab/LLM-Tabular-Shifts},
  selected={false}
}

@inproceedings{ChenLiChPeDoNa24,
  title={{QGym}: Scalable Simulation and Benchmarking of Queuing Network Controllers},
  author={Chen, Haozhe and Li, Ang and Che, Ethan and Peng, Tianyi and Dong, Jing and Namkoong, Hongseok},
  booktitle=nips2024db,
  year={2024},
  abbr=aid,
  bibtex_show={true},
  arxiv={2410.06170},
  url={https://arxiv.org/abs/2410.06170},
  code={https://github.com/namkoong-lab/QGym},
  selected={false}  
}

@article{CheDoNa24,
  title={Differentiable Discrete Event Simulation for Queuing Network Control},
  author={Che, Ethan and Dong, Jing and Namkoong, Hongseok},
  journal={arXiv:2409.03740 [cs.LG]},
  year={2024},
  abbr=aid,
  bibtex_show={true},
  arxiv={2409.03740},
  url={https://arxiv.org/abs/2409.03740},
  selected={false},
  note={Major revision in Operations Research},
  code={https://github.com/DifferentiableQueue/QueueTorch}
}

@inproceedings{ZolloSiYeLiNa24,
  title={{PersonalLLM}: Tailoring {LLMs} to Individual Preferences},
  author={Zollo$*$, Thomas and Siah$*$, Andrew and Ye, Naimeng and Li, Ang and Namkoong, Hongseok},
  booktitle={In International Conference on Learning Representations, 2025},
  year={2025},
  abbr=tai,
  bibtex_show={true},
  arxiv={2409.20296},
  url={https://www.arxiv.org/abs/2409.20296},
  code={https://github.com/namkoong-lab/PersonalLLM},
  selected={false}
}

@article{CheJiNaWa24,
  title={Optimization-Driven Adaptive Experimentation},
  author={Che, Ethan and Jiang, Daniel and Namkoong, Hongseok and Wang, Jimmy},
  journal={arXiv:2408.04570 [cs.LG]},
  year={2024},
  note={Selected for oral presentations at the Econometric Society
                  Interdisciplinary Frontiers: Economics and AI+ML
                  conference and Conference on Digital Experimentation},
  abbr=aid,
  bibtex_show={true},
  arxiv={2408.04570},
  url={https://arxiv.org/abs/2408.04570},
  code={https://github.com/namkoong-lab/aexgym},
  slides={CheJiNaWa24-slides.pdf},
  selected={true},
  abstract={Adaptivity
can significantly improve efficiency of experimentation, but it is challenging to implement even at large
online platforms with mature experimentation systems. 
As a result, many real-world
experiments are deliberately implemented with large batches and a handful of
opportunities to update the sampling allocation as a way to reduce operational
costs of experimentation. 
<br>
In this work, we focus on adaptive experiments with limited adaptivity (short horizons T < 10). Bandit algorithms focusing on long-horizon settings are tailored to provide regret guarantees for each specific case, and  we find they often underperform
static A/B tests on practical problem instances with
batched feedback, non-stationarity, multiple objectives and constraints, and
personalization.
<br>
In response, we develop a mathematical programming framework for
developing adaptive experimentation algorithms. Instead of the
problem-specific research paradigm (akin to an optimization solver developed
for a particular linear program), we ask the modeler to write down a flexible
optimization formulation and use modern machine learning systems to
(heuristically) solve for adaptive designs. 
Since a naive formulation of the adaptive
experimentation problem as a dynamic program is intractable, 
we propose a batched view of the experimentation process. We model the uncertainty around 
batch-level sufficient
statistics necessary to make allocation decisions, instead of attempting to
model unit-level outcomes whose distributions are commonly unknown and leads
to intractable dynamic programs with combinatorial action spaces. 

<br>
Sequential Gaussian approximations is the main intellectual vehicle
powering our mathematical programming framework. CLT-based normal approximations are universal in statistical
inference, and a sequential variant we prove provides a simple optimization formulation that lends itself to modern computational tools. Through extensive empirical
evaluation, we observe that even a preliminary and heuristic solution
approach can provide major robustness benefits. Unlike bespoke methods (e.g.,
Thompson sampling variants), our mathematical programming framework provides
consistent gains over static randomized control trials and exhibits robust
performance across problem instances.}
}

@article{WangChJiNa24,
  title={{AExGym}: Benchmarks and Environments for Adaptive Experimentation},
    author={Wang, Jimmy and Che, Ethan and Jiang, Daniel and Namkoong, Hongseok},
  journal={arXiv:2408.04531 [cs.LG]},
  year={2024},
  abbr=aid,
  bibtex_show={true},
  arxiv={2408.04531},
  url={https://arxiv.org/abs/2408.04531},
  code={https://github.com/namkoong-lab/aexgym}
}

@article{YeNa24,
  title={Exchangeable Sequence Models Quantify Uncertainty Over Latent Concepts},
  author={Ye, Naimeng and Namkoong, Hongseok},
  journal={arXiv:2408.03307 [stat.ML]},
  year={2024},
  abbr=tai,
  bibtex_show={true},
  arxiv={2408.03307},
  url={https://arxiv.org/abs/2408.03307},
  selected=false,
  abstract={  AI models are omni-present yet extrapolate in unexpected ways, 
  posing a significant barrier to robust and fair systems. 
  Building AI systems that can articulate their own uncertainty has been 
  a longstanding challenge in ML,  such probabilistic reasoning capability 
  is key to bounding downside risk (e.g., delegating to human experts) and 
  continually improving system performance by gathering data to resolve uncertainty. 
  Despite recent advances in large language models, uncertainty quantification remains a 
  challenge, with methods attempting to leverage these deep neural networks—such as Bayesian 
  neural networks—frequently facing scalability limitations. 
  <br>
    This work takes an important conceptual step towards building large-scale 
  AI systems that can reason about uncertainty through natural language. 
  We revisit De Finetti’s view of uncertainty coming from missing observations rather 
  than latent parameters, which allows us to pose learning to do statistical inference 
  as a prediction problem involving masked inputs. This formal connection between 
  autoregressive generation with probabilistic reasoning allows pre-trained sequence 
  models to express their epistemic uncertainty on underlying concepts, and refine 
  their beliefs as they gather more information. 
<br>
    Our findings open a promising avenue for addressing uncertainty in complex, 
  data-rich settings in a scalable way. We are excited by how this work leverages 
  a timeless insight to inform a timely topic: guiding the next generation of AI systems. 
  <br>
    1. As internet data depletes, the pace of progress in LLM capabilities has been widely 
  observed to slow down (even in public media). This suggests that the limited 
  paradigm of pre-training on passively scraped web data has reached its full potential. 
  To move forward, the authors believe that the next generation of AI systems 
  must be able to understand tasks on which they suffer high uncertainty, and 
  actively gather data in order to continually improve their performance. 
  <br>
    2. Since scalable uncertainty quantification poses a key intellectual bottleneck, 
  we resolve this by going back to De Finetti’s insight developed in the 1920s. 
  We believe the connection between Bayesian inference and autoregressive generation provides 
  the groundwork for building LLMs with probabilistic reasoning capabilities.
 <br>
 <br>
    Taken together, our work showcases how principled scientific insights have the 
  potential to shape the design of even the largest scale AI systems.
  }
}

@article{CaiFoHoNa24,
  title={Constrained Learning for Causal Inference and Semiparametric Statistics},
  author={Cai$*$, Tiffany and Fonseca$*$, Yuri and Hou, Kaiwen and Namkoong, Hongseok},
  journal={arXiv:2405.09493 [stat.ML]},
  year={2024},
  abbr=rc,
  bibtex_show={true},
  arxiv={2405.09493},
  url={https://arxiv.org/abs/2405.09493},
  abstract={
  Causal inference provides the foundation of decision-making in sciences and industry alike, 
  and our work addresses a longstanding gap between practical performance and theoretical guarantees in 
  causal inference. Machine learning-based methods can provide a powerful way to control for confounding, 
  and the de facto standard approach is to use debiased estimators, which enjoy guarantees like statistical
efficiency and double robustness; examples include one-step
estimation (i.e. augmented inverse propensity weighting (AIPW)) and targeted
maximum likelihood estimation (TMLE). 
<br>
However, in practice, these estimators have been observed to be unstable when there is 
limited overlap between treatment and control, necessitating ad hoc adjustments 
such as truncating propensity scores. In contrast, naive plug-in estimators 
using an ML model can be more stable but lack these desirable asymptotic properties. 
This trade-off can make it difficult to choose an estimator and ultimately, 
to reach a conclusion regarding the treatment effect. 
<br>
We propose a novel framework that combines the best of both worlds: 
we derive the best plug-in estimator that is debiased, 
retaining the stability of plug-ins while enjoying statistical efficiency and double robustness. 
Our estimation framework is based on a constrained optimization problem and 
can incorporate flexible modern ML techniques, including controlling for text-based confounders 
using LLMs.  Empirically, we demonstrate our approach over a range of examples, 
and observe that it outperforms standard debiased methods when there is limited overlap.
<br>
As low overlap settings are a persistent challenge in practice, 
we expect these results will be of interest to a broad spectrum of researchers, 
including practitioners in statistics, economics, and machine learning. 
We are unusually excited by how our framework provides a novel and pragmatic approach 
to a longstanding challenge in causal inference.
By introducing an entirely new constrained optimization framework for semiparametric estimation, we hope to spur further progress in developing robust but theoretically grounded estimators.
  }
}

@article{CaiNaRuZh25,
  title={Active Exploration via Autoregressive Generation of Missing Data},
  author={Cai, Tiffany and Namkoong, Hongseok and Russo, Daniel and Zhang, Kelly},
  journal={arXiv:2405.19466 [cs.LG]},
  year={2025},
  note={Selected for presentation at the Econometric Society
                  Interdisciplinary Frontiers: Economics and AI+ML
                  conference},
  abbr=aid,
  bibtex_show={true},
  arxiv={2405.19466},
  url={https://arxiv.org/abs/2405.19466},
  slides={ZhangCaNaRu24-slides.pdf},
  selected={true}
}

@article{LeeNaZe24,
title = {Design and Scheduling of an AI-based Queueing System},
author = {Jiung Lee  and Hongseok Namkoong and Yibo Zeng},
year = 2024,
journal = {arXiv:2406.06855 [math.OC]},
arxiv={2406.06855},
abbr = aid,
bibtex_show={true},
abstract={Recent advances in AI present significant opportunities to
                  rethink the design of service systems with AI at the
                  forefront. Even in the era of LLMs, managing a
                  workforce of human agents (“servers”) is a crit-
                  ical problem. Crowdsourcing workers are vital for
                  aligning LLMs with human values (e.g., ChatGPT) and
                  in many domains, the cost of human annotation is a
                  binding constraint (e.g., medical diagnosis from
                  radiologists). This work models and analyzes modern
                  service systems involving human reviewers and
                  state-of-the-art AI models. A key intellectual
                  challenge in managing con- gestion within such
                  service systems is endogeneity. Prediction is never
                  the goal, and the link between predictive
                  performance and downstream decision-making
                  performance is not straightforward due to
                  endogeneity. Our work crystallizes how classical
                  tools from queueing theory provide managerial
                  insights into the design of AI-based service
                  systems.},
url={https://arxiv.org/abs/2406.06855},
note={Major revision in Management Science; Selected for presentation at SIG day 2025}
}

@article{LiuWaCuNa24,
title = {On the Need for a Language Describing Distribution Shifts: Illustrations on Tabular Datasets},
author = {Jiashuo Liu$*$ and Tianyu Wang$*$ and Peng Cui and Hongseok Namkoong},
year = 2024,
journal = {arXiv:2307.05284 [cs.LG]},
abbr = tai,
bibtex_show={true},
url={https://arxiv.org/abs/2307.05284},
arxiv={2307.05284},
Abstract={Different distribution shifts require different interventions, and algorithms must be grounded in the specific shifts they address. Advocating for an inductive approach to research on distributional robustness, we build an empirical testbed, "WhyShift", comprising of natural shifts across 5 tabular datasets and 60,000 model configurations encompassing imbalanced learning algorithms and distributionally robust optimization (DRO) methods. We find Y|X-shifts are most prevalent on our testbed, in stark contrast to the heavy focus on X (covariate)-shifts in the ML literature. We conduct
an in-depth empirical analysis of DRO methods and find that the underlying model class (e.g.,
neural networks, XGBoost) and hyperparameter selection have a first-order impact in practice
despite being overlooked by DRO researchers. To further bridge that gap between methodological
research and practice, we design case studies that illustrate how such a refined understanding of
distribution shifts can enhance both data-centric and algorithmic interventions.},
note={Major revision in Management Science; Conference version appeared in NeurIPS 2023},
slides={LiuWaCuNa24-slides.pdf},
website={https://github.com/namkoong-lab/whyshift},
video={https://gsb-columbia-edu.zoom.us/rec/play/sKowK1a0T6cGx8IJELKEISEZs9MbUPB58xtmOmBjs09MfLY3RjEmUQ16AHdGL8PaipX2KAkW_gr4XIMT.T1PcyxmM_7XVve5H?canPlayFromShare=true&from=share_recording_detail&continueMode=true&componentName=rec-play&originRequestUrl=https%3A%2F%2Fgsb-columbia-edu.zoom.us%2Frec%2Fshare%2F_Dqv3yqQl8Gj27n-c8j5Cdpp9MJPoQGRNIUyFHQGskOhGsa6KneLYgmH3Qx-ogK2.-Qp9jC6bs7_RCZWA},
  selected={false}
}

@article{CheNa23,
title = {Adaptive Experimentation at Scale: A Computational Framework for Flexible Batches},
author = {Ethan Che and Hongseok Namkoong},
year = 2023,
journal = {arXiv:2303.11582 [cs.LG]},
note={Major revision in Operations Research},
abbr = aid,
bibtex_show={true},
url={https://arxiv.org/abs/2303.11582},
arxiv={2303.11582},
abstract={Starting with my one-year stint at Meta's adaptive
                  experimentation team, I've been pondering on how
                  bandit algorithms are largely designed by
                  theoreticians to achieve good regret bounds and are
                  rarely used in practice due to the difficulty of
                  implementation and poor empirical performance. In
                  this work, we focus on underpowered, short-horizon,
                  and large-batch problems that typically arise in
                  practice. We use large batch normal approximations
                  to derive an MDP formulation for deriving the
                  optimal adaptive design. Our formulation allows the
                  use of computational tools for designing adaptive
                  algorithms, a break from the existing theory-driven
                  paradigm.
                  Our approach significantly improves statistical power over standard 
                  methods, even when compared to Bayesian bandit algorithms 
                  (e.g., Thompson sampling) that require full distributional knowledge 
                  of individual rewards. Overall, we expand the scope of 
                  adaptive experimentation to settings that are difficult 
                  for standard methods, involving limited adaptivity, 
                  low signal-to-noise ratio, and unknown reward distributions.},
video={https://youtu.be/CLzRcOw9eyk},
slides={CheNa23-slides.pdf},
website={https://aes-batch.streamlit.app/}
}

@article{CaiNaYa25,
title = {Diagnosing Model Performance Under Distribution Shift},
author = {Tiffany Cai and Hongseok Namkoong and Steve Yadlowsky},
year = 2025,
journal = {Operations Research},
note = {Conference version appeared Symposium on Foundations of Responsible Computing 2023},
abbr = tai,
bibtex_show={true},
url={https://arxiv.org/abs/2303.02011},
arxiv={2303.02011},
abstract={Recent advances in AI present significant opportunities to
                  rethink the design of service systems with AI at the
                  forefront. Even in the era of LLMs, managing a
                  workforce of human agents (“servers”) is a crit-
                  ical problem. Crowdsourcing workers are vital for
                  aligning LLMs with human values (e.g., ChatGPT) and
                  in many domains, the cost of human annotation is a
                  binding constraint (e.g., medical diagnosis from
                  radiologists). This work models and analyzes modern
                  service systems involving human reviewers and
                  state-of-the-art AI models. A key intellectual
                  challenge in managing con- gestion within such
                  service systems is endogeneity. Prediction is never
                  the goal, and the link between predictive
                  performance and downstream decision-making
                  performance is not straightforward due to
                  endogeneity. Our work crystallizes how classical
                  tools from queueing theory provide managerial
                  insights into the design of AI-based service
                  systems.}, video={https://gsb-columbia-edu.zoom.us/rec/play/sKowK1a0T6cGx8IJELKEISEZs9MbUPB58xtmOmBjs09MfLY3RjEmUQ16AHdGL8PaipX2KAkW_gr4XIMT.T1PcyxmM_7XVve5H?canPlayFromShare=true&from=share_recording_detail&continueMode=true&componentName=rec-play&originRequestUrl=https%3A%2F%2Fgsb-columbia-edu.zoom.us%2Frec%2Fshare%2F_Dqv3yqQl8Gj27n-c8j5Cdpp9MJPoQGRNIUyFHQGskOhGsa6KneLYgmH3Qx-ogK2.-Qp9jC6bs7_RCZWA},
 code={https://github.com/namkoong-lab/disde},
 slides={LiuWaCuNa24-slides.pdf},
  selected={false}
}

@article{BoyarskyNaPo23,
title = {Modeling Interference via Experiment Rollout},
author = {Ari Boyarsky and Hongseok Namkoong and Jean Pouget-Abadie},
year = 2023,
journal={arXiv:2305.10728 [stat.ME]},
note = {Conference version appeared in ACM conference on Economics and Computation},
abbr = rc,
bibtex_show={true},
url={https://arxiv.org/abs/2305.10728},
arxiv={2305.10728},
slides={BoyarskyNaPo23-slides.pdf}
}


@article{NamkoongDaBa24,
  title={Distilled Thompson Sampling: Practical and Efficient Thompson Sampling via Imitation Learning},
  author={Hongseok Namkoong$*$ and Samuel Daulton$*$ and Eytan Bakshy},
  journal={arXiv:2011.14266 [stat.ML]},
  year=2024,
  note={Selected for an oral presentation at the Neurips 2020 OfflineRL Workshop},
  abbr = aid,
  bibtex_show={true},
  url={https://arxiv.org/abs/2011.14266},
  arxiv={2011.14266}
}

@article{JeongNa22,
  title={Assessing External Validity via Worst-case Subpopulation Treatment Effects},
  author={Sookyo Jeong and  Hongseok Namkoong},
  journal={arXiv:2007.02411 [stat.ML]},
  year=2022,
note={Short version appeared in Conference on Learning Theory 2020; Major revision in Management Science},
abbr = rc,
bibtex_show={true},
url={https://arxiv.org/abs/2007.02411},
arxiv={2007.02411},
code={https://github.com/sookyojeong/worst-ate},
slides={JeongNa22-slides.pdf},
video={https://youtu.be/FqdH75RazNQ?si=x0H5IOyf-Iy8h7g4}
}

@article{LiNaXi24,
author = {Mike Li and Daksh Mittal and Hongseok Namkoong and Shangzhou Xia},
title = {Evaluating Model Performance Under Worst-case Subpopulations},
year = 2024,
journal = {arXiv:2407.01316 [cs.LG]},
  note={Major revision in Mathematics of Operations Research; Short version appeared in NeurIPS 2021},
  abbr=tai,
  url={https://arxiv.org/abs/2407.01316},
  arxiv={2407.01316}
}

@article{NamkoongMaGl25,
title = {Minimax Optimal Estimation of Stability Under Distribution
                  Shift},
author = {Hongseok Namkoong$*$ and Yuanzhe Ma$*$ and Peter W. Glynn},
year = 2025,
journal = {arXiv:2212.06338 [stat.ML]},
note={To appear in Operations Research},
abbr= tai,
url={https://arxiv.org/abs/2212.06338},
arxiv={2212.06338},
code={https://github.com/namkoong-lab/stability_estimation/}
}

@inproceedings{WortsmanIlGaRoGoMoNaFaCaKoSc22,
  title={Model Soups: Averaging Weights of Multiple Fine-tuned Models Improves Accuracy Without Increasing Inference Time},
  author={Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir Yitzhak and Roelofs, Rebecca and Gontijo-Lopes, Raphael and Morcos, Ari S and Namkoong, Hongseok and Farhadi, Ali and Carmon, Yair and Kornblith, Simon and Schmidt, Ludwig},
  booktitle=icml22,
  year={2022},
  abbr = tai,
bibtex_show={true},
url={https://proceedings.mlr.press/v162/wortsman22a/wortsman22a.pdf},
arxiv={2203.05482},
 code={https://github.com/mlfoundations/model-soups},
 video={https://www.youtube.com/live/brHeIKX8ayw?si=nzlq_6l8RUcQuLPq}
}

@inproceedings{WortsmanIlLiKiHaFaNaSc22,
  title={Robust Fine-tuning of Zero-shot Models},
  author={Wortsman$*$, Mitchell and Ilharco$*$, Gabriel and Kim, Jong Wook and Li, Mike and  Kornblith, Simon and  Roelofs, Rebecca and Gontijo-Lopes, Raphael and Hajishirzi, Hannaneh and Farhadi, Ali and Namkoong, Hongseok and Schmidt, Ludwig},
  booktitle=cvpr22,
  note={CVPR Best Paper Finalist},
  year={2022},
  abbr = tai,
bibtex_show={true},
url={https://openaccess.thecvf.com/content/CVPR2022/papers/Wortsman_Robust_Fine-Tuning_of_Zero-Shot_Models_CVPR_2022_paper.pdf},
arxiv={2109.01903},
video={https://www.youtube.com/live/brHeIKX8ayw?si=nzlq_6l8RUcQuLPq},
 code={https://github.com/mlfoundations/wise-ft}
}

@article{YadlowskyNaBaDuTi22,
  title={Bounds on the Conditional and Average Treatment Effect 
                  with Unobserved Confounding Factors},
  author={Steve Yadlowsky and Hongseok Namkoong and Sanjay Basu and
                  John Duchi and Lu Tian},
  journal=aos,
  volume={50},
  number=5,
  pages={2587--2615},
  year=2022,
  abbr=rc,
bibtex_show={true},
url={https://projecteuclid.org/journals/annals-of-statistics/volume-50/issue-5/Bounds-on-the-conditional-and-average-treatment-effect-with-unobserved/10.1214/22-AOS2195.full},
  arxiv={1808.09521},
  video={https://youtu.be/FqdH75RazNQ?si=x0H5IOyf-Iy8h7g4},
  slide={YadlowskyNaBaDuTi22-slides.pdf}
}

@inproceedings{NamkoongKeYaBr20,
  title={Off-policy Policy Evaluation For Sequential Decisions Under Unobserved Confounding},
  author={Hongseok Namkoong$*$ and Ramtin Keramati$*$ and Steve Yadlowsky$*$ and Emma Brunskill},
  booktitle=nips2020,
  year={2020},
 abbr=rc,
bibtex_show={true},
url={https://proceedings.neurips.cc/paper/2020/file/da21bae82c02d1e2b8168d57cd3fbab7-Paper.pdf},
code={https://github.com/StanfordAI4HI/off_policy_confounding},
slide={YadlowskyNaBaDuTi22-slides.pdf}
}

@article{DuchiHaNa22,
author = {John C. Duchi and Tatsunori Hashimoto and Hongseok Namkoong},
title = {Distributionally Robust Losses Against Mixture Covariate Shifts},
year = 2022,
journal = {Operations Research},
bibtex_show={true},
abbr=tai,
url={https://pubsonline.informs.org/doi/10.1287/opre.2022.2363},
arxiv={2007.13982},
video={https://youtu.be/DRlF5sdCkKY?si=U8J_zUzrPaRAoNLj},
code={https://github.com/namkoong-lab/marginal-dro}
}

@article{DuchiNa21,
author = {John C. Duchi and Hongseok Namkoong},
title = {Learning Models with Uniform Performance via Distributionally
                  Robust Optimization},
year = {2021},
volume= 49,
number = 3,
pages={1378-1406},
journal = {Annals of Statistics},
bibtex_show={true},
abbr=tai,
url={https://projecteuclid.org/journals/annals-of-statistics/volume-49/issue-3/Learning-models-with-uniform-performance-via-distributionally-robust-optimization/10.1214/20-AOS2004.full},
arxiv={1810.08750},
code={https://github.com/namkoong-lab/chi-squared-dro},
slides={dro-slides.pdf},
video={https://youtu.be/DRlF5sdCkKY?si=U8J_zUzrPaRAoNLj}
}

@article{DuchiGlNa21,
  title={Statistics of Robust Optimization: A Generalized Empirical
    Likelihood Approach},
  author={John C. Duchi and Peter W. Glynn and Hongseok Namkoong},
  year={2021},
  volume={46},
  number=3,
  pages={946-969},
  journal={Mathematics of Operations Research},
bibtex_show={true},
  note={APS Best Student Paper Prize},
  abbr=tai,
  url={https://pubsonline.informs.org/doi/10.1287/moor.2020.1085},
  arxiv={1610.03425},
  video={https://youtu.be/JBrrRFwi-yg?si=X1_GxFwSLfJviu4N}
}

@inproceedings{SinhaNaVoDu18,
  title={Certifiable Distributional Robustness with Principled Adversarial Training},
  author={Sinha$*$, Aman and Namkoong$*$, Hongseok and Volpi, Riccardo and Duchi, John},
  booktitle={International Conference on Learning Representations},
  year=2018,
  note={Selected for a full oral presentation; 2\% of submissions},
bibtex_show={true},
  abbr=tai,
  url={https://arxiv.org/abs/1710.10571},
  arxiv={1710.10571},
  code={https://github.com/duchi-lab/certifiable-distributional-robustness},
  video={https://youtu.be/JBrrRFwi-yg?si=X1_GxFwSLfJviu4N}
}

@article{DuchiNa19,
title = {Variance-based regularization with convex objectives},
author = {John C. Duchi and Hongseok Namkoong},
year = 2019,
journal = {Journal of Machine Learning Research},
note={Conference version won NeurIPS 2017 Best Paper Award},
abbr=tai,
bibtex_show={true},
url={https://jmlr.csail.mit.edu/papers/volume20/17-750/17-750.pdf},
arxiv={1610.02581},
video={https://youtu.be/2j0rrgr4bUc?si=3l_TQYLDPdwnnIW6},
slide={NamkoongDu17-slides.pdf},
code={https://github.com/namkoong-lab/chi-squared-dro},
}

@inproceedings{VolpiNaSeDuMuSa18,
  title={Generalizing to Unseen Domains via Adversarial Data Augmentation},
  author={Volpi$*$, Riccardo and Namkoong$*$, Hongseok and Duchi,
                  John and Murino, Vittorio and Savarese, Silvio},
  booktitle=nips2018,
  year=2018,
  abbr=tai,
  bibtex_show={true},
  url={https://proceedings.neurips.cc/paper_files/paper/2018/file/1d94108e907bb8311d8802b48fd54b4a-Paper.pdf},
  arxiv={1805.12018},
  code={https://github.com/ricvolpi/adversarial-feature-augmentation}
}

@inproceedings{OKellySiNaDuTe18,
  title={Scalable End-to-End Autonomous Vehicle Testing via Rare-event Simulation},
  author={O'Kelly$*$, Mathew and Sinha$*$, Aman and Namkoong$*$, Hongseok
          and Duchi, John and Tedrake, Russ},
  booktitle=nips2018,
  year=2018,
  abbr=tai,
  bibtex_show={true},
  url={https://proceedings.neurips.cc/paper_files/paper/2018/file/653c579e3f9ba5c03f2f2f8cf4512b39-Paper.pdf},
  arxiv={1811.00145}
}


@inproceedings{HashimotoSrNaLi18,
  title={Fairness Without Demographics in Repeated Loss Minimization},
  author={Hashimoto, Tatsunori and Srivastava, Megha and Namkoong,
                  Hongseok  and Liang, Percy},
  booktitle={International Conference on Machine Learning},
  year=2018,
  note={Best Paper Runner-up Award},
  abbr=tai,
  bibtex_show={true},
  url={https://proceedings.mlr.press/v80/hashimoto18a/hashimoto18a.pdf},
  arxiv={1806.08010},
  code={https://bit.ly/2sFkDpE}
}

@inproceedings{NamkoongSiYaDu17,
  title={Adaptive sampling probabilities for non-smooth optimization},
  author={Namkoong, Hongseok and Sinha, Aman and Yadlowsky, Steve and Duchi, John C},
  booktitle={International Conference on Machine Learning},
  pages={2574--2583},
  year={2017},
  bibtex_show={true},
  url={https://proceedings.mlr.press/v70/namkoong17a/namkoong17a.pdf},
  code={https://github.com/duchi-lab/adaptive-sampling-descent}
}

@inproceedings{NamkoongDu16,
author = {Hongseok Namkoong and John C. Duchi},
title = {Stochastic Gradient Methods for Distributionally
  Robust Optimization with $f$-divergences},
year = 2016,
booktitle = nips2016,
bibtex_show={true},
abbr=tai,
url={https://papers.nips.cc/paper_files/paper/2016/hash/4588e674d3f0faf985047d4c3f13ed0d-Abstract.html},
slide={NamkoongDu16-slides.pdf}
}




